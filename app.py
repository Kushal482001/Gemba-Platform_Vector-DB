# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1495C0dIWQ-KUG91ov044XBPpsH9MkWDt
"""

# Commented out IPython magic to ensure Python compatibility.
import streamlit as st
import pandas as pd
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chat_models import AzureChatOpenAI

# ================================
# üîê Azure OpenAI Configuration
# ================================
AZURE_API_KEY = st.secrets["AZURE_API_KEY"]
AZURE_ENDPOINT = st.secrets["AZURE_ENDPOINT"]
AZURE_API_VERSION = st.secrets["AZURE_API_VERSION"]
CHAT_DEPLOYMENT_NAME = st.secrets["CHAT_DEPLOYMENT_NAME"]  # ‚úÖ Chat model only

# ================================
# üß† Load and Embed Excel Data
# ================================
def load_xlsx_as_db(filepath):
    df = pd.read_excel(filepath)
    docs = []
    for _, row in df.iterrows():
        content = "\n".join([f"{col}: {row[col]}" for col in df.columns if pd.notnull(row[col])])
        docs.append(Document(page_content=content))
    chunks = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}
    )
    return FAISS.from_documents(chunks, embeddings)

# ================================
# üéõÔ∏è Streamlit UI
# ================================
st.title("üîç Suntory Concept Generator")
query = st.text_input("Describe your issue, challenge, or idea:")

if query:
    # Load vector databases
    gemba_db = load_xlsx_as_db("Gemba Issues Data.xlsx")
    startup_db = load_xlsx_as_db("Tech Data.xlsx")
    ops_db = load_xlsx_as_db("Ops Flow Data.xlsx")

    st.write(f"üìä Gemba DB: {len(gemba_db.index_to_docstore_id)} docs")
    st.write(f"üìä Startup DB: {len(startup_db.index_to_docstore_id)} docs")
    st.write(f"üìä Ops Flow DB: {len(ops_db.index_to_docstore_id)} docs")

    # Retrieve top 3 relevant chunks
    gemba = gemba_db.similarity_search(query, k=3)
    startup = startup_db.similarity_search(query, k=3)
    ops = ops_db.similarity_search(query, k=3)

    # Merge context
    g_text = "\n".join([doc.page_content for doc in gemba])
    s_text = "\n".join([doc.page_content for doc in startup])
    o_text = "\n".join([doc.page_content for doc in ops])

    # Setup LLM
    llm = AzureChatOpenAI(
        deployment_name=CHAT_DEPLOYMENT_NAME,
        openai_api_key=AZURE_API_KEY,
        openai_api_base=AZURE_ENDPOINT,
        openai_api_type="azure",
        openai_api_version=AZURE_API_VERSION,
        temperature=1,
    )

    # Build Prompt
    prompt = f"""
You are an innovation consultant at Suntory. Based on the following information, propose an innovation concept that addresses real-world challenges and leverages existing technologies across domains.

---
Gemba Issues:
{g_text}

Startup/Tech Solutions:
{s_text}

Operations Process Flows:
{o_text}
---

Your output should include the following:

1. **Concept Summary**: A brief overview of the proposed innovation.
2. **Key Features**: Detailed description of each core feature.
3. **Traceability Section**:
   - For each key feature, list:
     - Which Gemba issue(s) it addresses
     - Which Startup/Tech solution(s) inspired it
     - Which part(s) of the Ops Flow it applies to
4. **Benefits**: Specific, measurable business or user benefits.
5. **Suggested Next Steps**: Pilot plan, stakeholder involvement, partner suggestions, and rollout roadmap.

Keep the tone practical and strategic ‚Äî focus on feasible, impactful solutions that can be pitched to management and cross-functional teams.
"""


    # Generate Response
    response = llm.invoke(prompt)

    # Show result
    st.subheader("üí° Innovation Concept")
    st.write(response.content)


#from pyngrok import ngrok
import os
import time
import threading
import subprocess

# Set OpenAI key (optional if already in app.py)
os.environ["OPENAI_API_KEY"] = st.secrets["AZURE_API_KEY"]

# Kill any existing ngrok tunnels
#ngrok.kill()

# Set your ngrok auth token
#ngrok.set_auth_token("308KJTypEv4LojS9HM3Iu5UrBoA_3qFWHTXNhUwcgngobNzyu")

# Run Streamlit in a thread so it doesn't block
def run():
    subprocess.call(["streamlit", "run", "app.py"])

thread = threading.Thread(target=run)
thread.start()

# Wait for streamlit to start
time.sleep(5)

# Create ngrok tunnel
#tunnel = ngrok.connect(8501)
